{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Welcome to exercise one of week four of \u201cApache Spark for Scalable Machine Learning on BigData\u201d. In this exercise we\u2019ll work on classification.\n\nLet\u2019s create our DataFrame again:\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "This notebook is designed to run in a IBM Watson Studio Apache Spark runtime. In case you are running it in an IBM Watson Studio standard runtime or outside Watson Studio, we install Apache Spark in local mode for test purposes only. Please don't use it in production."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip install --upgrade pip",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "if not ('sc' in locals() or 'sc' in globals()):\n    print('It seems you are note running in a IBM Watson Studio Apache Spark Notebook. You might be running in a IBM Watson Studio Default Runtime or outside IBM Waston Studio. Therefore installing local Apache Spark environment for you. Please do not use in Production')\n    \n    from pip import main\n    main(['install', 'pyspark==2.4.5'])\n    \n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\n\n    sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n    \n    spark = SparkSession \\\n        .builder \\\n        .getOrCreate()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# delete files from previous runs\n!rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n    \n# create a dataframe out of it\ndf = spark.read.parquet('hmp.parquet')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')",
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200412144937-0000\nKERNEL_ID = 2991d032-bc7a-4882-b47f-8dc620227c80\n--2020-04-12 14:49:40--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 140.82.113.4\nConnecting to github.com (github.com)|140.82.113.4|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n--2020-04-12 14:49:40--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n--2020-04-12 14:49:40--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: 'hmp.parquet'\n\n100%[======================================>] 932,997     --.-K/s   in 0.04s   \n\n2020-04-12 14:49:41 (21.3 MB/s) - 'hmp.parquet' saved [932997/932997]\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Since this is supervised learning, let\u2019s split our data into train (80%) and test (20%) set."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "splits = df.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]",
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Again, we can re-use our feature engineering pipeline"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.feature import StringIndexer, OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\n\nindexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n                                  outputCol=\"features\")\n\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)",
            "execution_count": 3,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Now we use LogisticRegression, a simple and basic linear classifier to obtain a classification performance baseline."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\npipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer,lr])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)",
            "execution_count": 5,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "If we look at the schema of the prediction dataframe we see that there is an additional column called prediction which contains the best guess for the class our model predicts."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "prediction.printSchema()",
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "root\n |-- x: integer (nullable = true)\n |-- y: integer (nullable = true)\n |-- z: integer (nullable = true)\n |-- source: string (nullable = true)\n |-- class: string (nullable = true)\n |-- label: double (nullable = false)\n |-- features: vector (nullable = true)\n |-- features_norm: vector (nullable = true)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = false)\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Let\u2019s evaluate performance by using a build-in functionality of Apache SparkML."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nMulticlassClassificationEvaluator().setMetricName(\"accuracy\").evaluate(prediction) ",
            "execution_count": 7,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 7,
                    "data": {
                        "text/plain": "0.20695915616889987"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "So we get 20% right. This is not bad for a baseline. Note that random guessing would give us only 7%. Of course we need to improve. You might have notices that we\u2019re dealing with a time series here. And we\u2019re not making use of that fact right now as we look at each training example only individually. But this is ok for now. More advanced courses like \u201cAdvanced Machine Learning and Signal Processing\u201d (https://www.coursera.org/learn/advanced-machine-learning-signal-processing/) will teach you how to improve accuracy to the nearly 100% by using algorithms like Fourier transformation or wavelet transformation. But let\u2019s skip this for now. In the following cell, please use the RandomForest classifier (you might need to play with the \u201cnumTrees\u201d parameter) in the code cell below. You should get an accuracy of around 44%. More on RandomForest can be found here:\n\nhttps://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\nfrom pyspark.ml.feature import IndexToString\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nindexer = StringIndexer(inputCol=\"class\",outputCol=\"label\").fit(df)\n\n# Create vector assembler then normalize vector values\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"], outputCol=\"features\")\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\")\n\nfeatureIndexer = VectorIndexer(inputCol=\"eatures_norm\", outputCol=\"indexedFeatures\")\n\n# Split the dataset \n# Split the data into training and test sets (30% held out for testing)\nsplits = df.randomSplit([0.8,0.2])\ntrain_data = splits[0]\ntest_data = splits[1]\n\n# Train the data \nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"indexedFeatures\", numTrees=10)\n\n# Convert indexed labels back to original labels.\nindexToString = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=indexer.labels)\n\n",
            "execution_count": 31,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#piapline = Pipeline(stages=[indexer, vectorAssembler, normalizer, rf, indexToString])\n\npipline = Pipeline(stages=[indexer, vectorAssembler, normalizer, featureIndexer , rf, indexToString])\n\n\n# Train the model\nmodel = pipeline.fit(train_data)\n\n# Make predictions\npredictions = model.transform(test_data)",
            "execution_count": 33,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "predictions.show()",
            "execution_count": 34,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+--------------+-----+---------------+--------------------+--------------------+--------------------+----------+\n|  x|  y|  z|              source|         class|label|       features|       features_norm|       rawPrediction|         probability|prediction|\n+---+---+---+--------------------+--------------+-----+---------------+--------------------+--------------------+--------------------+----------+\n|  0| 23| 36|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,23.0,36.0]|[0.0,0.3898305084...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 24| 35|Accelerometer-201...| Sitdown_chair|  8.0|[0.0,24.0,35.0]|[0.0,0.4067796610...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 25| 40|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,25.0,40.0]|[0.0,0.3846153846...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 27| 37|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,27.0,37.0]|[0.0,0.421875,0.5...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 29| 34|Accelerometer-201...|          Walk|  0.0|[0.0,29.0,34.0]|[0.0,0.4603174603...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 29| 38|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,29.0,38.0]|[0.0,0.4328358208...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 29| 39|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,29.0,39.0]|[0.0,0.4264705882...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 30| 24|Accelerometer-201...| Standup_chair|  7.0|[0.0,30.0,24.0]|[0.0,0.5555555555...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 30| 38|Accelerometer-201...|  Climb_stairs|  4.0|[0.0,30.0,38.0]|[0.0,0.4411764705...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 30| 43|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,30.0,43.0]|[0.0,0.4109589041...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 31| 29|Accelerometer-201...|          Walk|  0.0|[0.0,31.0,29.0]|[0.0,0.5166666666...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 31| 30|Accelerometer-201...| Standup_chair|  7.0|[0.0,31.0,30.0]|[0.0,0.5081967213...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 31| 32|Accelerometer-201...| Standup_chair|  7.0|[0.0,31.0,32.0]|[0.0,0.4920634920...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 31| 34|Accelerometer-201...|Descend_stairs| 10.0|[0.0,31.0,34.0]|[0.0,0.4769230769...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 31| 38|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,31.0,38.0]|[0.0,0.4492753623...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 32| 33|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,32.0,33.0]|[0.0,0.4923076923...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 32| 33|Accelerometer-201...|  Climb_stairs|  4.0|[0.0,32.0,33.0]|[0.0,0.4923076923...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 33| 27|Accelerometer-201...|Descend_stairs| 10.0|[0.0,33.0,27.0]|     [0.0,0.55,0.45]|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 33| 31|Accelerometer-201...|Descend_stairs| 10.0|[0.0,33.0,31.0]|[0.0,0.515625,0.4...|[1.25477692694303...|[0.20665460802587...|       0.0|\n|  0| 33| 40|Accelerometer-201...|Descend_stairs| 10.0|[0.0,33.0,40.0]|[0.0,0.4520547945...|[1.25477692694303...|[0.20665460802587...|       0.0|\n+---+---+---+--------------------+--------------+-----+---------------+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[3]\nprint(rfModel)  # summary only",
            "execution_count": 35,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Test Error = 0.793607\nLogisticRegression_49198ced00b3145ec995\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"class\", outputCol=\"indexedLabel\").fit(df)\n\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"], outputCol=\"features\")\n\npip = Pipeline(stages=[labelIndexer, vectorAssembler])\nmod = pip.fit(df)\npred = mod.transform(df)\n\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(pred)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = pred.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n                               labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=[featureIndexer, rf, labelConverter])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n",
            "execution_count": 36,
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "AnalysisException",
                    "evalue": "\"cannot resolve '`label`' given input columns: [x, indexedLabel, features, predictedLabel, rawPrediction, z, source, class, probability, y, prediction, indexedFeatures];;\\n'Project [predictedLabel#1607, 'label, features#1517]\\n+- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, features#1517, indexedFeatures#1554, rawPrediction#1563, probability#1573, prediction#1584, if (isnull(cast(prediction#1584 as double))) null else UDF(knownotnull(cast(prediction#1584 as double))) AS predictedLabel#1607]\\n   +- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, features#1517, indexedFeatures#1554, rawPrediction#1563, probability#1573, UDF(rawPrediction#1563) AS prediction#1584]\\n      +- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, features#1517, indexedFeatures#1554, rawPrediction#1563, UDF(rawPrediction#1563) AS probability#1573]\\n         +- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, features#1517, indexedFeatures#1554, UDF(indexedFeatures#1554) AS rawPrediction#1563]\\n            +- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, features#1517, UDF(features#1517) AS indexedFeatures#1554]\\n               +- Sample 0.7, 1.0, false, 7589409949849962762\\n                  +- Sort [x#0 ASC NULLS FIRST, y#1 ASC NULLS FIRST, z#2 ASC NULLS FIRST, source#3 ASC NULLS FIRST, class#4 ASC NULLS FIRST, indexedLabel#1507 ASC NULLS FIRST, features#1517 ASC NULLS FIRST], false\\n                     +- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, UDF(named_struct(x_double_VectorAssembler_44d9970a1513d68ea99b, cast(x#0 as double), y_double_VectorAssembler_44d9970a1513d68ea99b, cast(y#1 as double), z_double_VectorAssembler_44d9970a1513d68ea99b, cast(z#2 as double))) AS features#1517]\\n                        +- Project [x#0, y#1, z#2, source#3, class#4, UDF(cast(class#4 as string)) AS indexedLabel#1507]\\n                           +- Relation[x#0,y#1,z#2,source#3,class#4] parquet\\n\"",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
                        "\u001b[0;32m/opt/ibm/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2257.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`label`' given input columns: [x, indexedLabel, features, predictedLabel, rawPrediction, z, source, class, probability, y, prediction, indexedFeatures];;\n'Project [predictedLabel#1607, 'label, features#1517]\n+- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, features#1517, indexedFeatures#1554, rawPrediction#1563, probability#1573, prediction#1584, if (isnull(cast(prediction#1584 as double))) null else UDF(knownotnull(cast(prediction#1584 as double))) AS predictedLabel#1607]\n   +- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, features#1517, indexedFeatures#1554, rawPrediction#1563, probability#1573, UDF(rawPrediction#1563) AS prediction#1584]\n      +- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, features#1517, indexedFeatures#1554, rawPrediction#1563, UDF(rawPrediction#1563) AS probability#1573]\n         +- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, features#1517, indexedFeatures#1554, UDF(indexedFeatures#1554) AS rawPrediction#1563]\n            +- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, features#1517, UDF(features#1517) AS indexedFeatures#1554]\n               +- Sample 0.7, 1.0, false, 7589409949849962762\n                  +- Sort [x#0 ASC NULLS FIRST, y#1 ASC NULLS FIRST, z#2 ASC NULLS FIRST, source#3 ASC NULLS FIRST, class#4 ASC NULLS FIRST, indexedLabel#1507 ASC NULLS FIRST, features#1517 ASC NULLS FIRST], false\n                     +- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, UDF(named_struct(x_double_VectorAssembler_44d9970a1513d68ea99b, cast(x#0 as double), y_double_VectorAssembler_44d9970a1513d68ea99b, cast(y#1 as double), z_double_VectorAssembler_44d9970a1513d68ea99b, cast(z#2 as double))) AS features#1517]\n                        +- Project [x#0, y#1, z#2, source#3, class#4, UDF(cast(class#4 as string)) AS indexedLabel#1507]\n                           +- Relation[x#0,y#1,z#2,source#3,class#4] parquet\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:84)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:84)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3301)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1312)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:819)\n",
                        "\nDuring handling of the above exception, another exception occurred:\n",
                        "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-36-e7c30ffcd176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Select example rows to display.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predictedLabel\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Select (prediction, true label) and compute test error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/ibm/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \"\"\"\n\u001b[0;32m-> 1202\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/ibm/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`label`' given input columns: [x, indexedLabel, features, predictedLabel, rawPrediction, z, source, class, probability, y, prediction, indexedFeatures];;\\n'Project [predictedLabel#1607, 'label, features#1517]\\n+- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, features#1517, indexedFeatures#1554, rawPrediction#1563, probability#1573, prediction#1584, if (isnull(cast(prediction#1584 as double))) null else UDF(knownotnull(cast(prediction#1584 as double))) AS predictedLabel#1607]\\n   +- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, features#1517, indexedFeatures#1554, rawPrediction#1563, probability#1573, UDF(rawPrediction#1563) AS prediction#1584]\\n      +- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, features#1517, indexedFeatures#1554, rawPrediction#1563, UDF(rawPrediction#1563) AS probability#1573]\\n         +- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, features#1517, indexedFeatures#1554, UDF(indexedFeatures#1554) AS rawPrediction#1563]\\n            +- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, features#1517, UDF(features#1517) AS indexedFeatures#1554]\\n               +- Sample 0.7, 1.0, false, 7589409949849962762\\n                  +- Sort [x#0 ASC NULLS FIRST, y#1 ASC NULLS FIRST, z#2 ASC NULLS FIRST, source#3 ASC NULLS FIRST, class#4 ASC NULLS FIRST, indexedLabel#1507 ASC NULLS FIRST, features#1517 ASC NULLS FIRST], false\\n                     +- Project [x#0, y#1, z#2, source#3, class#4, indexedLabel#1507, UDF(named_struct(x_double_VectorAssembler_44d9970a1513d68ea99b, cast(x#0 as double), y_double_VectorAssembler_44d9970a1513d68ea99b, cast(y#1 as double), z_double_VectorAssembler_44d9970a1513d68ea99b, cast(z#2 as double))) AS features#1517]\\n                        +- Project [x#0, y#1, z#2, source#3, class#4, UDF(cast(class#4 as string)) AS indexedLabel#1507]\\n                           +- Relation[x#0,y#1,z#2,source#3,class#4] parquet\\n\""
                    ]
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Select example rows to display.\npredictions.show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only",
            "execution_count": 37,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-------------+------------+---------------+---------------+--------------------+--------------------+----------+--------------+\n|  x|  y|  z|              source|        class|indexedLabel|       features|indexedFeatures|       rawPrediction|         probability|prediction|predictedLabel|\n+---+---+---+--------------------+-------------+------------+---------------+---------------+--------------------+--------------------+----------+--------------+\n|  0| 10| 28|Accelerometer-201...|    Getup_bed|         1.0|[0.0,10.0,28.0]|[0.0,10.0,28.0]|[2.06820818114962...|[0.20682081811496...|       1.0|     Getup_bed|\n|  0| 12| 39|Accelerometer-201...|Sitdown_chair|         8.0|[0.0,12.0,39.0]|[0.0,12.0,39.0]|[2.44930119655347...|[0.24493011965534...|       1.0|     Getup_bed|\n|  0| 26| 15|Accelerometer-201...| Climb_stairs|         4.0|[0.0,26.0,15.0]|[0.0,26.0,15.0]|[2.06820818114962...|[0.20682081811496...|       1.0|     Getup_bed|\n|  0| 26| 42|Accelerometer-201...|  Brush_teeth|         6.0|[0.0,26.0,42.0]|[0.0,26.0,42.0]|[2.05595919610202...|[0.20559591961020...|       1.0|     Getup_bed|\n|  0| 27| 31|Accelerometer-201...|Sitdown_chair|         8.0|[0.0,27.0,31.0]|[0.0,27.0,31.0]|[2.34374704576039...|[0.23437470457603...|       1.0|     Getup_bed|\n+---+---+---+--------------------+-------------+------------+---------------+---------------+--------------------+--------------------+----------+--------------+\nonly showing top 5 rows\n\nTest Error = 0.563349\nIndexToString_404889abbd1dcf4542c3\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python36",
            "display_name": "Python 3.6 with Spark",
            "language": "python3"
        },
        "language_info": {
            "mimetype": "text/x-python",
            "nbconvert_exporter": "python",
            "name": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8",
            "file_extension": ".py",
            "codemirror_mode": {
                "version": 3,
                "name": "ipython"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}