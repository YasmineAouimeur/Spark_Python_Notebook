{
    "cells": [
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!git clone https://github.com/wchill/HMP_Dataset.git",
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200407102721-0000\nKERNEL_ID = 1d439fdf-0247-4f34-931c-fbb819dabfb6\nCloning into 'HMP_Dataset'...\nremote: Enumerating objects: 865, done.\u001b[K\nremote: Total 865 (delta 0), reused 0 (delta 0), pack-reused 865\u001b[K\nReceiving objects: 100% (865/865), 1010.96 KiB | 0 bytes/s, done.\nChecking out files: 100% (848/848), done.\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#check the content of the folder HMP_Dataset\n!ls HMP_Dataset",
            "execution_count": 21,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Brush_teeth\tDrink_glass  Liedown_bed  Sitdown_chair  final.py\r\nClimb_stairs\tEat_meat     MANUAL.txt   Standup_chair  impdata.py\r\nComb_hair\tEat_soup     Pour_water   Use_telephone\r\nDescend_stairs\tGetup_bed    README.txt   Walk\r\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!ls HMP_Dataset/Brush_teeth\n\nos.listdir('HMP_Dataset/Brush_teeth')",
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt\r\nAccelerometer-2011-04-11-13-29-54-brush_teeth-f1.txt\r\nAccelerometer-2011-05-30-08-35-11-brush_teeth-f1.txt\r\nAccelerometer-2011-05-30-09-36-50-brush_teeth-f1.txt\r\nAccelerometer-2011-05-30-10-34-16-brush_teeth-m1.txt\r\nAccelerometer-2011-05-30-21-10-57-brush_teeth-f1.txt\r\nAccelerometer-2011-05-30-21-55-04-brush_teeth-m2.txt\r\nAccelerometer-2011-05-31-15-16-47-brush_teeth-f1.txt\r\nAccelerometer-2011-06-02-10-42-22-brush_teeth-f1.txt\r\nAccelerometer-2011-06-02-10-45-50-brush_teeth-f1.txt\r\nAccelerometer-2011-06-06-10-45-27-brush_teeth-f1.txt\r\nAccelerometer-2011-06-06-10-48-05-brush_teeth-f1.txt\r\n",
                    "name": "stdout"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 4,
                    "data": {
                        "text/plain": "['Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt',\n 'Accelerometer-2011-04-11-13-29-54-brush_teeth-f1.txt',\n 'Accelerometer-2011-05-30-08-35-11-brush_teeth-f1.txt',\n 'Accelerometer-2011-05-30-09-36-50-brush_teeth-f1.txt',\n 'Accelerometer-2011-05-30-10-34-16-brush_teeth-m1.txt',\n 'Accelerometer-2011-05-30-21-10-57-brush_teeth-f1.txt',\n 'Accelerometer-2011-05-30-21-55-04-brush_teeth-m2.txt',\n 'Accelerometer-2011-05-31-15-16-47-brush_teeth-f1.txt',\n 'Accelerometer-2011-06-02-10-42-22-brush_teeth-f1.txt',\n 'Accelerometer-2011-06-02-10-45-50-brush_teeth-f1.txt',\n 'Accelerometer-2011-06-06-10-45-27-brush_teeth-f1.txt',\n 'Accelerometer-2011-06-06-10-48-05-brush_teeth-f1.txt']"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.sql.types import StructType, StructField, IntegerType\n\n## Create a data structure or the data schema \n\nschema = StructType([\n    StructField('x', IntegerType(), True),\n    StructField('y', IntegerType(), True),\n    StructField('z', IntegerType(), True)\n])\n",
            "execution_count": 5,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import os\n \n## extract content of the main folder \nfile_list = os.listdir('HMP_Dataset')\nfile_list",
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 6,
                    "data": {
                        "text/plain": "['.git',\n '.idea',\n 'Brush_teeth',\n 'Climb_stairs',\n 'Comb_hair',\n 'Descend_stairs',\n 'Drink_glass',\n 'Eat_meat',\n 'Eat_soup',\n 'Getup_bed',\n 'Liedown_bed',\n 'MANUAL.txt',\n 'Pour_water',\n 'README.txt',\n 'Sitdown_chair',\n 'Standup_chair',\n 'Use_telephone',\n 'Walk',\n 'final.py',\n 'impdata.py']"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "## Select only dataset classes (folder names) and filter files\nfile_list_filtred = [s for s in file_list if '_' in s]",
            "execution_count": 7,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "file_list_filtred",
            "execution_count": 8,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 8,
                    "data": {
                        "text/plain": "['Brush_teeth',\n 'Climb_stairs',\n 'Comb_hair',\n 'Descend_stairs',\n 'Drink_glass',\n 'Eat_meat',\n 'Eat_soup',\n 'Getup_bed',\n 'Liedown_bed',\n 'Pour_water',\n 'Sitdown_chair',\n 'Standup_chair',\n 'Use_telephone']"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "## Load data \n\n\n## Initialize an empty data frame\ndf = None\n\n\nfrom pyspark.sql.functions import lit\n\n## Read the content (files) of each folder\nfor category in file_list_filtred:\n    data_files = os.listdir('HMP_Dataset/'+category)\n    \n    ## read the content of each file in a temporary dataframe (temp_df), each file contains data structred as the defined schema (x, y, z)\n    for data_file in data_files:\n        ##print(data_file)\n        temp_df = spark.read.option(\"header\",\"false\").option(\"delimiter\", \" \").csv('HMP_Dataset/'+category+'/'+data_file, schema = schema)\n        \n        ## Add a two columns 'class' containing the category and 'source' containing the file name\n        temp_df = temp_df.withColumn('class',lit(category))\n        temp_df = temp_df.withColumn('source',lit(data_file))\n        \n        if df is None:\n            df = temp_df\n        else:\n            df = df.union(temp_df)",
            "execution_count": 9,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\ndf.show()",
            "execution_count": 10,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+\n|  x|  y|  z|      class|              source|\n+---+---+---+-----------+--------------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|\n+---+---+---+-----------+--------------------+\nonly showing top 20 rows\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "## Transform Data\n\n## Crate a STRING INDEXER in order to transform string classes to a number of integers inorder to permit to machine learning algorithms to cope with it\nfrom pyspark.ml.feature import StringIndexer\n\nindexer = StringIndexer(inputCol=\"class\",outputCol=\"classIndexer\")\n\nindexed = indexer.fit(df).transform(df)\n\nindexed.show()",
            "execution_count": 11,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+------------+\n|  x|  y|  z|      class|              source|classIndexer|\n+---+---+---+-----------+--------------------+------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|         5.0|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|         5.0|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|         5.0|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|         5.0|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|         5.0|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|         5.0|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|         5.0|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|         5.0|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|         5.0|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|         5.0|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|         5.0|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|         5.0|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|         5.0|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|         5.0|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|         5.0|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|         5.0|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|         5.0|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|         5.0|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|         5.0|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|         5.0|\n+---+---+---+-----------+--------------------+------------+\nonly showing top 20 rows\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "## We use ONE-HOT-ENCODER inorder to transform a single column number value to a vector containing 12 columns and with 11 zeros and one column contain \n## \"one\" \nfrom pyspark.ml.feature import OneHotEncoder\n\nencoder = OneHotEncoder(inputCol=\"classIndexer\",outputCol=\"categoryVec\")\n\nencoded = encoder.transform(indexed)\n\n## , you see here, we have created categoryvec and this is the one-hot encoded vector,So, it's a compressed vector that means here, this 12 means it\n## has 12 elements and it says here at position three that is a one. So, if you would have a second one index sparse vector at position five, then it\n## would look like the following. \nencoded.show()",
            "execution_count": 13,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+------------+--------------+\n|  x|  y|  z|      class|              source|classIndexer|   categoryVec|\n+---+---+---+-----------+--------------------+------------+--------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|\n+---+---+---+-----------+--------------------+------------+--------------+\nonly showing top 20 rows\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "##We have to transform our values X, Y, Z into vectors, because Spark ML only can work on vector objects so let's actually do that. So, let's import \n## vectors and the vector assembler. \n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\nVectorAssembler = VectorAssembler(inputCols=['x','y','z'],outputCol='features')\n\nfeatures_vectorized = VectorAssembler.transform(encoded)\n\nfeatures_vectorized.show()",
            "execution_count": 14,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+------------+--------------+----------------+\n|  x|  y|  z|      class|              source|classIndexer|   categoryVec|        features|\n+---+---+---+-----------+--------------------+------------+--------------+----------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[21.0,52.0,34.0]|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,51.0,34.0]|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[20.0,50.0,35.0]|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,52.0,34.0]|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,50.0,34.0]|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,51.0,35.0]|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[21.0,51.0,33.0]|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[20.0,50.0,34.0]|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[20.0,51.0,35.0]|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[18.0,49.0,34.0]|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[19.0,48.0,34.0]|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[16.0,53.0,34.0]|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[18.0,52.0,35.0]|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[18.0,51.0,32.0]|\n+---+---+---+-----------+--------------------+------------+--------------+----------------+\nonly showing top 20 rows\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.feature import Normalizer\n\nNormalizer = Normalizer(inputCol='features', outputCol='features_norm')\n\nnormalized = Normalizer.transform(features_vectorized)\n\nnormalized.show()",
            "execution_count": 15,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+------------+--------------+----------------+--------------------+\n|  x|  y|  z|      class|              source|classIndexer|   categoryVec|        features|       features_norm|\n+---+---+---+-----------+--------------------+------------+--------------+----------------+--------------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|[0.34316403829308...|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|[0.34316403829308...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|[0.33117360613218...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|[0.33117360613218...|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[21.0,52.0,34.0]|[0.32020976616922...|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,51.0,34.0]|[0.33782247905081...|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[20.0,50.0,35.0]|[0.31139957766460...|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,52.0,34.0]|[0.33379342096892...|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,50.0,34.0]|[0.34191842968811...|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,51.0,35.0]|[0.33510742122295...|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[21.0,51.0,33.0]|[0.32673201960653...|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[20.0,50.0,34.0]|[0.31403714651066...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|[0.33494058369652...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|[0.33494058369652...|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[20.0,51.0,35.0]|[0.30765590086407...|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[18.0,49.0,34.0]|[0.28893535304210...|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[19.0,48.0,34.0]|[0.30737255142631...|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[16.0,53.0,34.0]|[0.24627045148779...|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[18.0,52.0,35.0]|[0.27600999787655...|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[18.0,51.0,32.0]|[0.28643688243157...|\n+---+---+---+-----------+--------------------+------------+--------------+----------------+--------------------+\nonly showing top 20 rows\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\nfrom pyspark.ml import Pipeline\npipeline = Pipeline(stages=[indexer,encoder,VectorAssembler,Normalizer])\nmodel = pipeline.fit(df)\nprediction = model.transform(df)",
            "execution_count": 18,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "prediction.show()",
            "execution_count": 19,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+------------+--------------+----------------+--------------------+\n|  x|  y|  z|      class|              source|classIndexer|   categoryVec|        features|       features_norm|\n+---+---+---+-----------+--------------------+------------+--------------+----------------+--------------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|[0.34316403829308...|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|[0.34316403829308...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|[0.33117360613218...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|[0.33117360613218...|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[21.0,52.0,34.0]|[0.32020976616922...|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,51.0,34.0]|[0.33782247905081...|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[20.0,50.0,35.0]|[0.31139957766460...|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,52.0,34.0]|[0.33379342096892...|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,50.0,34.0]|[0.34191842968811...|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[22.0,51.0,35.0]|[0.33510742122295...|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[21.0,51.0,33.0]|[0.32673201960653...|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[20.0,50.0,34.0]|[0.31403714651066...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|[0.33494058369652...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|[0.33494058369652...|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[20.0,51.0,35.0]|[0.30765590086407...|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[18.0,49.0,34.0]|[0.28893535304210...|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[19.0,48.0,34.0]|[0.30737255142631...|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[16.0,53.0,34.0]|[0.24627045148779...|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[18.0,52.0,35.0]|[0.27600999787655...|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|         5.0|(12,[5],[1.0])|[18.0,51.0,32.0]|[0.28643688243157...|\n+---+---+---+-----------+--------------------+------------+--------------+----------------+--------------------+\nonly showing top 20 rows\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df_train = prediction.drop(\"source\").drop(\"class\").drop(\"classIndex\").drop(\"features\").drop(\"x\").drop(\"y\").drop(\"z\")",
            "execution_count": 20,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+------------+--------------+--------------------+\n|classIndexer|   categoryVec|       features_norm|\n+------------+--------------+--------------------+\n|         5.0|(12,[5],[1.0])|[0.34316403829308...|\n|         5.0|(12,[5],[1.0])|[0.34316403829308...|\n|         5.0|(12,[5],[1.0])|[0.33117360613218...|\n|         5.0|(12,[5],[1.0])|[0.33117360613218...|\n|         5.0|(12,[5],[1.0])|[0.32020976616922...|\n|         5.0|(12,[5],[1.0])|[0.33782247905081...|\n|         5.0|(12,[5],[1.0])|[0.31139957766460...|\n|         5.0|(12,[5],[1.0])|[0.33379342096892...|\n|         5.0|(12,[5],[1.0])|[0.34191842968811...|\n|         5.0|(12,[5],[1.0])|[0.33510742122295...|\n|         5.0|(12,[5],[1.0])|[0.32673201960653...|\n|         5.0|(12,[5],[1.0])|[0.31403714651066...|\n|         5.0|(12,[5],[1.0])|[0.33494058369652...|\n|         5.0|(12,[5],[1.0])|[0.33494058369652...|\n|         5.0|(12,[5],[1.0])|[0.30765590086407...|\n|         5.0|(12,[5],[1.0])|[0.28893535304210...|\n|         5.0|(12,[5],[1.0])|[0.30737255142631...|\n|         5.0|(12,[5],[1.0])|[0.24627045148779...|\n|         5.0|(12,[5],[1.0])|[0.27600999787655...|\n|         5.0|(12,[5],[1.0])|[0.28643688243157...|\n+------------+--------------+--------------------+\nonly showing top 20 rows\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df_train.show()",
            "execution_count": 22,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+------------+--------------+--------------------+\n|classIndexer|   categoryVec|       features_norm|\n+------------+--------------+--------------------+\n|         5.0|(12,[5],[1.0])|[0.34316403829308...|\n|         5.0|(12,[5],[1.0])|[0.34316403829308...|\n|         5.0|(12,[5],[1.0])|[0.33117360613218...|\n|         5.0|(12,[5],[1.0])|[0.33117360613218...|\n|         5.0|(12,[5],[1.0])|[0.32020976616922...|\n|         5.0|(12,[5],[1.0])|[0.33782247905081...|\n|         5.0|(12,[5],[1.0])|[0.31139957766460...|\n|         5.0|(12,[5],[1.0])|[0.33379342096892...|\n|         5.0|(12,[5],[1.0])|[0.34191842968811...|\n|         5.0|(12,[5],[1.0])|[0.33510742122295...|\n|         5.0|(12,[5],[1.0])|[0.32673201960653...|\n|         5.0|(12,[5],[1.0])|[0.31403714651066...|\n|         5.0|(12,[5],[1.0])|[0.33494058369652...|\n|         5.0|(12,[5],[1.0])|[0.33494058369652...|\n|         5.0|(12,[5],[1.0])|[0.30765590086407...|\n|         5.0|(12,[5],[1.0])|[0.28893535304210...|\n|         5.0|(12,[5],[1.0])|[0.30737255142631...|\n|         5.0|(12,[5],[1.0])|[0.24627045148779...|\n|         5.0|(12,[5],[1.0])|[0.27600999787655...|\n|         5.0|(12,[5],[1.0])|[0.28643688243157...|\n+------------+--------------+--------------------+\nonly showing top 20 rows\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df.printSchema()",
            "execution_count": 23,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "root\n |-- x: integer (nullable = true)\n |-- y: integer (nullable = true)\n |-- z: integer (nullable = true)\n |-- class: string (nullable = false)\n |-- source: string (nullable = false)\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\nimport ibmos2spark\n# @hidden_cell\ncredentials = {\n    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'service_id': 'iam-ServiceId-ea077ea4-9c80-49c6-86bd-a1850ae92a5d',\n    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token',\n    'api_key': 'Hqn4G20dLSvKzwoY8zLu_RVJ5tguL9LZa6cVcvna3TY4'\n}\n\nconfiguration_name = 'os_31daaa1631b34a279477ff52936c86c0_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n# Please read the documentation of 'SparkSession.read()' to learn more about the possibilities to adjust the data loading.\n# PySpark documentation: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json\n\n# df_data_1 = spark.read.json(cos.url('floorsensordata2604.json', 'sparktuto-donotdelete-pr-jsw0wchjtrvwr5'))\n# df_data_1.take(5)\n\n# we repartition it to one partition in order to make sure that we only create one file.\ndf = df.repartition(1)\ndf.write.parquet(cos.url('test.parquet', 'sparktuto-donotdelete-pr-jsw0wchjtrvwr5'))\n",
            "execution_count": 25,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# let's actually load the data frame from object store, just to make sure that everything works.\n# So it's spark.read.parquet and then URL, so we copy it from here.\n\ndf_from_object_store = spark.read.parquet(cos.url('test.parquet', 'sparktuto-donotdelete-pr-jsw0wchjtrvwr5'))\n\n# We notice that reading the DF from the object store is faster than loading the data file from desktop to \n# to the object store",
            "execution_count": 26,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df_from_object_store.show()",
            "execution_count": 27,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+\n|  x|  y|  z|      class|              source|\n+---+---+---+-----------+--------------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|\n+---+---+---+-----------+--------------------+\nonly showing top 20 rows\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python36",
            "display_name": "Python 3.6 with Spark",
            "language": "python3"
        },
        "language_info": {
            "mimetype": "text/x-python",
            "nbconvert_exporter": "python",
            "name": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8",
            "file_extension": ".py",
            "codemirror_mode": {
                "version": 3,
                "name": "ipython"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}